---
title: Prova
date: 2025-09-09
categories: [TOP_CATEGORY, SUB_CATEGORY]
tags: [machine learning]
description: Short summary of the postss.
author: <autore_DC> 
math: true
---


## Introduzione alla Regressione Lineare Semplice

Benvenuti in questo post introduttivo dove esploreremo uno dei concetti fondamentali del Machine Learning: la **Regressione Lineare Semplice**. Questo algoritmo, nonostante la sua semplicità, è una base cruciale per comprendere modelli più complessi e ci permette di prevedere una variabile continua a partire da un'altra.

Immaginiamo di voler prevedere il prezzo di una casa ($y$) basandoci sulla sua dimensione ($x$). La regressione lineare cerca di trovare una relazione lineare tra queste due variabili.

### Il Modello Matematico

Il cuore della regressione lineare è una semplice equazione che descrive una linea retta. In termini matematici, possiamo esprimerla come:

$$
y = \beta_0 + \beta_1 x + \epsilon
$$

Dove:
* $y$ è la **variabile dipendente** (quella che vogliamo prevedere, es. prezzo della casa).
* $x$ è la **variabile indipendente** (quella che usiamo per la previsione, es. dimensione della casa).
* $\beta_0$ è l'**intercetta** (il valore di $y$ quando $x=0$).
* $\beta_1$ è il **coefficiente di pendenza** (indica di quanto cambia $y$ per ogni unità di cambiamento in $x$).
* $\epsilon$ (epsilon) rappresenta l'**errore irriducibile** o rumore. È la parte di $y$ che il nostro modello non riesce a spiegare.

### Obiettivo: Minimizzare l'Errore

Il nostro obiettivo è trovare i valori ottimali di $\beta_0$ e $\beta_1$ che facciano sì che la linea "si adatti" al meglio ai nostri dati. Ma cosa significa "adattarsi al meglio"? Significa minimizzare la differenza tra i valori $y$ reali e i valori $\hat{y}$ (y-hat) previsti dal nostro modello.

Il valore previsto si calcola come:

$$
\hat{y}_i = \beta_0 + \beta_1 x_i
$$

Per quantificare l'errore totale del modello, usiamo una metrica chiamata **Somma degli Errori Quadratici (SSE - Sum of Squared Errors)**, anche nota come **Residual Sum of Squares (RSS)** o **Mean Squared Error (MSE)** quando divisa per il numero di osservazioni. La formula per l'SSE è:

$$
\text{SSE} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

Dove $n$ è il numero di osservazioni nei nostri dati.

Per trovare i migliori $\beta_0$ e $\beta_1$, dobbiamo minimizzare questa funzione SSE. Questo si fa usando il calcolo differenziale, impostando le derivate parziali rispetto a $\beta_0$ e $\beta_1$ uguali a zero. I risultati finali, dopo la derivazione, ci danno le formule per calcolare i coefficienti ottimali:

$$
\begin{equation}
  \beta_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}
  \label{eq:beta1}
\end{equation}
$$

$$
\begin{equation}
  \beta_0 = \bar{y} - \beta_1 \bar{x}
  \label{eq:beta0}
\end{equation}
$$

Possiamo fare riferimento a queste equazioni come l'equazione per la pendenza \eqref{eq:beta1} e per l'intercetta \eqref{eq:beta0}.

### Conclusione

La regressione lineare è un punto di partenza eccellente nel mondo del Machine Learning. Comprendere come funziona, dal modello matematico alla minimizzazione dell'errore tramite MSE (Mean Squared Error), è fondamentale. Sebbene semplice, è alla base di molti concetti più avanzati e offre un primo assaggio della potenza della statistica e dell'algebra lineare applicate ai dati.

Spero che questo post ti abbia fornito una chiara introduzione. Prossimamente, potremmo esplorare come implementare questo modello in Python o discutere altre metriche di valutazione come $R^2$.